{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985cbbd6-717e-4433-a411-982a78a95546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "## GPT-J Êñá‰ª∂Ë∑ØÂæÑ\n",
    "tokenizer = AutoTokenizer.from_pretrained('Instruct_gpt_J')\n",
    "generator = AutoModelForCausalLM.from_pretrained(\"Instruct_gpt_J\", max_length = 512, temperature=1, torch_dtype=torch.float16).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16699fc0-bfad-4cd5-96f7-62c1a405bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_dic = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f0bb6c-1160-4682-b567-c6c7575804f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import string\n",
    "import argparse\n",
    "\n",
    "def data_helper(file_dir):\n",
    "    idx = 0\n",
    "    sentence_list = []\n",
    "    \n",
    "    ## labelÁöÑÈ°∫Â∫èÊòØÂõ∫ÂÆöÁöÑ \n",
    "    ### CrossTest Dataset\n",
    "    if 'ai' in file_dir:\n",
    "        labels = [\"Field\", \"Task\", \"Conference\", \"Misc\", \"Product\", \"Programlang\", \"Organisation\", \"Algorithm\", \"Researcher\", \"Metrics\", \"University\", \"Country\", \"Person\", \"Location\"]\n",
    "    elif 'literature' in file_dir:\n",
    "        labels = ['Book', 'Writer', 'Award', 'Misc', 'Organisation', 'Person', 'Literarygenre', 'Poem', 'Event', 'Country', 'Location', 'Magazine']\n",
    "    elif 'music' in file_dir:\n",
    "        labels = ['Award', 'Album', 'Band', 'Musicalartist', 'Musicgenre', 'Organisation', 'Song', 'Location', 'Event', 'Misc', 'Country', 'Person', 'Musicalinstrument']\n",
    "    elif 'politics' in file_dir:\n",
    "        labels = ['Politicalparty', 'Election', 'Organisation', 'Politician', 'Event', 'Person', 'Location', 'Misc', 'Country']\n",
    "    elif 'science' in file_dir:\n",
    "        labels = ['Organisation', 'Scientist', 'Misc', 'Award', 'Astronomicalobject', 'Academicjournal', 'University', 'Chemicalcompound', 'Person', 'Location', 'Protein', 'Event', 'Enzyme', 'Discipline', 'Country', 'Chemicalelement', 'Theory']\n",
    "    ### ConLL2003 including Typos0 and OOV\n",
    "    # elif 'conll03' in file_dir:\n",
    "    ### OntoNotes5\n",
    "    \n",
    "    prompt = \"Extract the entities in the text to the entity type \" \n",
    "\n",
    "    # Open the file in read mode\n",
    "    with tqdm(total = os.path.getsize(file_dir)) as pbar:\n",
    "        with open(file_dir, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n') \n",
    "                if 'context' in line:\n",
    "                    idx += 1 \n",
    "                    line = str(line)\n",
    "                    sentence = line.split(\": \" )\n",
    "                    for label in labels:\n",
    "                        context = sentence[1][:-1]\n",
    "                        query = prompt + label + \":\\n\" + context\n",
    "                        # print(query)\n",
    "                        inputs = tokenizer(query, return_tensors='pt')\n",
    "                        outputs = generator.generate(inputs.input_ids.cuda(), pad_token_id=tokenizer.eos_token_id)\n",
    "                        outputs_str = (tokenizer.decode(outputs[0]))\n",
    "                        result = (outputs_str.split(\"\\n\" )[2])\n",
    "                        sentence_dic[context].append(result)\n",
    "                pbar.update(len(line)) \n",
    "                pass\n",
    "                \n",
    "            print(\"number_sentence:\", idx)\n",
    "            return sentence_dic, labels\n",
    "\n",
    "def get_entity(line, context):\n",
    "    line = line.split(\": \" )[-1]\n",
    "    # res = line.split(\", \")\n",
    "    if '\\ ' in line:\n",
    "        res = re.split('\\|, |\"', line)\n",
    "    else:\n",
    "        res = line.split(\", \")\n",
    "\n",
    "    out = []\n",
    "\n",
    "    for i in res:\n",
    "        tmp = str.maketrans({key: None for key in string.punctuation})\n",
    "        j = i.translate(tmp)\n",
    "        if j != '' and j.lower() in context.lower():\n",
    "            out.append(j)\n",
    "            \n",
    "        res_entity = ','.join(out)\n",
    "    return res_entity\n",
    "\n",
    "def get_result(ground_file, sentence_dic, labels):\n",
    "    with open(ground_file, 'r') as f:\n",
    "        all_datas = []\n",
    "        for line in f.readlines():\n",
    "            line = line.strip('\\n')\n",
    "            if 'context' in line:\n",
    "                line = str(line)\n",
    "                sentence = line.split(\": \" )\n",
    "                key = (sentence[1][:-1])\n",
    "                # print(key)\n",
    "                # assert key in sentence_dic\n",
    "                results = list(sentence_dic[key])\n",
    "                pos = {}\n",
    "                # print((results))\n",
    "                # assert len(labels) == len(results)\n",
    "                for i in range (len(results)):\n",
    "                    entity_result = get_entity(results[i], key)\n",
    "                    if i < len(labels):\n",
    "                        key_entity = str(labels[i]).lower()\n",
    "                        pos[key_entity] = entity_result\n",
    "                one_samp = {\n",
    "                    \"context\": key,\n",
    "                    \"entity\": pos}\n",
    "                all_datas.append(one_samp)\n",
    "        return(all_datas)\n",
    "\n",
    "\n",
    "# dataset dir ÈúÄË¶ÅÁªôÂá∫ÊµãËØïË∑ØÂæÑ\n",
    "real_dir = 'dataset/politics.test'\n",
    "print(\"get dataset!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8815de06-88a1-4a98-b58a-eb668f999e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating results...\")\n",
    "\n",
    "sentence_dic, labels = data_helper(real_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9022aaba-ae3e-41fc-a52c-e1b10ecadedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get results\n",
    "all_datas = get_result(real_dir, sentence_dic, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b1db30-8791-455e-ae2b-e04eb6b4f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save resutls\n",
    "# pred_dirÈúÄË¶ÅÊ†πÊçÆÊµãËØïÁöÑÊï∞ÊçÆÈõÜÊñá‰ª∂Êîπ‰∏Ä‰∏ãË∑ØÂæÑÔºåÂ¶Ç xxx_result.test\n",
    "pred_dir = \"xxx_result.test\"\n",
    "with open(pred_dir, \"w\") as f:\n",
    "    json.dump(all_datas, f, sort_keys=False, ensure_ascii=False, indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20869f29-a193-468c-b83e-3d6fbc6c3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(real_dir, pred_dir):\n",
    "    all_test_data = json.load(open(pred_dir, encoding=\"utf-8\"))\n",
    "    all_ground_data = json.load(open(real_dir, encoding=\"utf-8\"))\n",
    "\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    print(len(all_ground_data))\n",
    "    print(len(all_test_data))\n",
    "\n",
    "    for idx in range (len(all_ground_data)):\n",
    "        test_data = all_test_data[idx]\n",
    "        ground_data = all_ground_data[idx]\n",
    "        pred_idxLab = test_data[\"entity\"]\n",
    "        real_idxLab = ground_data[\"entity\"]\n",
    "\n",
    "        assert len(pred_idxLab) == len(real_idxLab)\n",
    "        for key,value in real_idxLab.items():\n",
    "            assert key in pred_idxLab\n",
    "            if value != None and pred_idxLab[key] != '' and pred_idxLab[key] != ' ':\n",
    "                real_value = str(value).split(',ÔΩú, | ,')\n",
    "                pred_value = str(pred_idxLab[key]).split(',')\n",
    "\n",
    "                for i in range(len(real_value)):   # ÈÅçÂéÜlist‰∏≠ÁöÑÊØè‰∏Ä‰∏™ÂÄº\n",
    "                    real_value[i] = real_value[i].strip(' ').lower()\n",
    "                    real_value[i] = real_value[i].lower()\n",
    "                for j in range(len(pred_value)):   # ÈÅçÂéÜlist‰∏≠ÁöÑÊØè‰∏Ä‰∏™ÂÄº\n",
    "                    pred_value[j] = pred_value[j].strip(' ')\n",
    "                    pred_value[j] = pred_value[j].lower()\n",
    "\n",
    "                tp += len(list(set(real_value).intersection(set(pred_value))))\n",
    "                fn += len(list(set(real_value).difference(set(pred_value))))\n",
    "                fp += len(list(set(pred_value).difference(set(real_value))))\n",
    "                \n",
    "            if value == None and pred_idxLab[key] != '' and pred_idxLab[key] != ' ':\n",
    "                pred_value = str(pred_idxLab[key]).split(',')\n",
    "                for j in range(len(pred_value)):   # ÈÅçÂéÜlist‰∏≠ÁöÑÊØè‰∏Ä‰∏™ÂÄº\n",
    "                    pred_value[j] = pred_value[j].strip(' ')\n",
    "                    pred_value[j] = pred_value[j].lower()\n",
    "                fp += len(pred_value)\n",
    "                \n",
    "            if value != None and (pred_idxLab[key] == '' or pred_idxLab[key] == ' '):\n",
    "                pred_value = str(pred_idxLab[key]).split(',')\n",
    "                for j in range(len(pred_value)):   # ÈÅçÂéÜlist‰∏≠ÁöÑÊØè‰∏Ä‰∏™ÂÄº\n",
    "                    pred_value[j] = pred_value[j].strip(' ')\n",
    "                    pred_value[j] = pred_value[j].lower()\n",
    "                fn += len(pred_value)\n",
    "\n",
    "    return tp, fn, fp\n",
    "\n",
    "\n",
    "print(\"evaluating...\")\n",
    "### evaluate results\n",
    "tp, fn, fp = metrics(real_dir, pred_dir)\n",
    "precision = tp/(tp+fp+1e-5) \n",
    "recall = tp/(tp+fn+1e-5)\n",
    "f1 = 2*precision*recall/(precision+recall+1e-5)\n",
    "print('| [ENTITY] precision: {0:3.4f}, recall: {1:3.4f}, f1: {2:3.4f}'.format(precision, recall, f1) + '\\r')  \n",
    "\n",
    "print('finished! üéâ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f49108c-ac82-42ba-baef-993cf3d7158c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
